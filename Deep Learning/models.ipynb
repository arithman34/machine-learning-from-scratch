{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers=None):\n",
    "        self.layers = layers if layers is not None else []\n",
    "        self.loss = None\n",
    "        self.optimiser = None  # TODO - Research Adam optimiser\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_prop(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward_prop(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward_prop(self, dvalues):\n",
    "        for layer in reversed(self.layers):\n",
    "            dvalues = layer.backward_prop(dvalues)\n",
    "\n",
    "            self.optimiser.update_params(layer)\n",
    "\n",
    "    def compile(self, loss, optimiser):\n",
    "        self.loss = loss\n",
    "        self.optimiser = optimiser\n",
    "\n",
    "    def fit(self, X, y, epochs=1, batch_size=32):\n",
    "        # Number of samples\n",
    "        m = X.shape[0]\n",
    "        history = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            \n",
    "            epoch_loss = 0  # Initialise the epoch loss\n",
    "            for start_idx in range(0, m, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, m)\n",
    "                batch_X = X[start_idx:end_idx]\n",
    "                batch_y = y[start_idx:end_idx]\n",
    "                \n",
    "                # Forward propagation\n",
    "                output = self.forward_prop(batch_X)\n",
    "                \n",
    "                # Compute the loss\n",
    "                loss = self.loss.loss(batch_y, output)\n",
    "\n",
    "                # Accumulate the batch loss\n",
    "                epoch_loss += loss * (end_idx - start_idx)\n",
    "                \n",
    "                # Compute the gradient of the loss with respect to the output\n",
    "                dvalues = self.loss.derivative(batch_y, output)\n",
    "                \n",
    "                # Backward propagation\n",
    "                self.backward_prop(dvalues)\n",
    "            \n",
    "            # Compute average loss\n",
    "            epoch_loss /= m\n",
    "\n",
    "            # Store loss in history\n",
    "            if history.get('loss') is None:\n",
    "                history['loss'] = [epoch_loss]\n",
    "            else:\n",
    "                history['loss'].append(epoch_loss)\n",
    "\n",
    "            # Print the loss\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Batches: {round(m / batch_size)}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Evaluate the model\n",
    "        return self.forward_prop(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
